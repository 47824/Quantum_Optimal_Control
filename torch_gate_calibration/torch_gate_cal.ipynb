{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc45931e-f117-412a-b9ca-c65738633a7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Qiskit imports\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('/Users/arthurstrauss/Library/CloudStorage/OneDrive-NationalUniversityofSingapore/Coding_projects/Quantum_Optimal_Control'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "# Qiskit imports\n",
    "from qiskit import pulse, transpile\n",
    "from qiskit.transpiler import InstructionDurations\n",
    "from qiskit.circuit import ParameterVector, QuantumCircuit, QuantumRegister, Gate, \\\n",
    "    ParameterExpression, CircuitInstruction\n",
    "# from qiskit.circuit.random import random_circuit\n",
    "# from qiskit_ibm_provider import IBMProvider\n",
    "from qiskit_ibm_runtime import QiskitRuntimeService\n",
    "from qiskit.providers import Backend\n",
    "from qiskit.providers.fake_provider import FakeJakartaV2, FakeJakarta\n",
    "from qiskit_aer import AerSimulator\n",
    "from qiskit.circuit.library import XGate, CXGate\n",
    "\n",
    "from qconfig import QiskitConfig\n",
    "from quantumenvironment import QuantumEnvironment\n",
    "from torch_quantum_environment import TorchQuantumEnvironment\n",
    "from gymnasium.spaces import Box, Space\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import time\n",
    "from typing import Union, Optional, List, Sequence\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e1b780-6e09-4b4f-8f0c-b1709942392c",
   "metadata": {},
   "source": [
    "# Circuit macros for environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d868a9f-014c-4842-9168-93fd2efa69bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def custom_pulse_schedule(backend: Backend, qubit_tgt_register: Union[List[int], QuantumRegister],\n",
    "                          params: Union[Sequence[ParameterExpression], ParameterVector],\n",
    "                          default_schedule: Optional[Union[pulse.ScheduleBlock, pulse.Schedule]] = None):\n",
    "    \"\"\"\n",
    "    Define parametrization of the pulse schedule characterizing the target gate\n",
    "        :param backend: IBM Backend on which schedule shall be added\n",
    "        :param qubit_tgt_register: Qubit register on which\n",
    "        :param params: Parameters of the Schedule\n",
    "        :param default_schedule:  baseline from which one can customize the pulse parameters\n",
    "\n",
    "        :return: Parametrized Schedule\n",
    "    \"\"\"\n",
    "\n",
    "    if default_schedule is None:  # No baseline pulse, full waveform builder\n",
    "        pass\n",
    "    else:\n",
    "\n",
    "        # Look here for the pulse features to specifically optimize upon, for the x gate here, simply retrieve relevant\n",
    "        # parameters for the Drag pulse\n",
    "        pulse_ref = default_schedule.instructions[0][1].pulse\n",
    "\n",
    "        with pulse.build(backend=backend, name='param_schedule') as parametrized_schedule:\n",
    "\n",
    "            pulse.play(pulse.Drag(duration=pulse_ref.duration, amp=params[0], sigma=pulse_ref.sigma,\n",
    "                                  beta=pulse_ref.beta, angle=pulse_ref.angle),\n",
    "                       channel=pulse.DriveChannel(qubit_tgt_register[0]))\n",
    "\n",
    "        return parametrized_schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47a42969-7541-4c0d-8415-e4b83408dbc6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def param_circuit(qc: QuantumCircuit,\n",
    "              params: Optional[ParameterVector]=None, q_reg: Optional[QuantumRegister] = None):\n",
    "# To build a unique gate identifier, choose the name based on the input circuit name (by default and if used\n",
    "# with TFQuantumEnvironment class, this circuit name is of the form c_circ_trunc_{i})\n",
    "# custom_gate_name = f\"{target['gate'].name}_{qc.name[-1]}\"\n",
    "# custom_gate = Gate(custom_gate_name, len(target[\"register\"]), params=params.params)\n",
    "# custom_sched = custom_pulse_schedule(backend=backend, qubit_tgt_register=physical_qubits, params=params,\n",
    "#                                      default_schedule=backend.target.get_calibration(target[\"gate\"].name,\n",
    "#                                                                                      tuple(physical_qubits)))\n",
    "# qc.add_calibration(custom_gate, physical_qubits, custom_sched)\n",
    "# qc.append(custom_gate, physical_qubits)\n",
    "    if params is None:\n",
    "        params = ParameterVector(\"lol\", n_actions)\n",
    "    my_qc = QuantumCircuit(2, name=\"custom_cx\")\n",
    "    my_qc.u(np.pi * params[0], np.pi * params[1], np.pi * params[2], 0)\n",
    "    my_qc.u(np.pi * params[3], np.pi * params[4], np.pi * params[5], 1)\n",
    "    my_qc.rzx(np.pi * params[6], 0, 1)\n",
    "    qc.append(my_qc.to_instruction(label=\"custom_cx\"), physical_qubits)\n",
    "    # qc.rx(2 * np.pi * params[0], physical_qubits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296601c4-aa0e-4e8b-9092-290ed0cf1e0b",
   "metadata": {},
   "source": [
    "# Definition of QuantumEnvironment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c6f2f3b-4338-4ff2-8539-de8fe475d60d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuantumEnvironment composed of 2, \n",
       "Defined target: gate (Instruction(name='cx', num_qubits=2, num_clbits=0, params=[]))\n",
       "Defined backend: 'aer_simulator(fake_jakarta),\n",
       "Abstraction level: circuit,"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "sampling_Paulis = 100\n",
    "N_shots = 1  # Number of shots for sampling the quantum computer for each action vector\n",
    "fake_backend = FakeJakartaV2()\n",
    "\n",
    "aer_backend = AerSimulator.from_backend(fake_backend, noise_model=None)\n",
    "#service = QiskitRuntimeService(channel='ibm_quantum', instance='ibm-q-nus/default/default')\n",
    "#runtime_backend = service.backend(\"ibmq_qasm_simulator\")\n",
    "backend = aer_backend\n",
    "target_gate = CXGate()\n",
    "physical_qubits = [0, 1]\n",
    "n_qubits = len(physical_qubits)\n",
    "target = {\"gate\": target_gate, 'register': physical_qubits}\n",
    "config = QiskitConfig(parametrized_circuit=param_circuit, backend=backend)\n",
    "\n",
    "q_env = QuantumEnvironment(target, \"circuit\", config)\n",
    "q_env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0728780-bee0-4fc5-9522-cedb5977fd18",
   "metadata": {},
   "source": [
    "# Definition of TorchQuantumEnvironment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52cf9a05-f483-4b4f-abd3-6f390d54dd40",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arthurstrauss/opt/anaconda3/envs/qiskit_env/lib/python3.9/site-packages/gymnasium/spaces/box.py:130: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  gym.logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n"
     ]
    }
   ],
   "source": [
    "# Circuit context\n",
    "seed = 10\n",
    "training_steps_per_gate = 3\n",
    "# target_circuit = transpile(random_circuit(4, depth=2, max_operands=2, seed=seed), backend)\n",
    "target_circuit = QuantumCircuit(2)\n",
    "target_circuit.h(0)\n",
    "target_circuit.cx(0,1)\n",
    "target_circuit.x(1)\n",
    "target_circuit.cx(0,1)\n",
    "target_circuit.h([0,1])\n",
    "\n",
    "target_circuit = transpile(target_circuit, backend, optimization_level=0)\n",
    "tgt_qubits = [target_circuit.qubits[i] for i in physical_qubits]\n",
    "\n",
    "tgt_instruction_counts = target_circuit.data.count(CircuitInstruction(target_gate, tgt_qubits))\n",
    "\n",
    "batchsize = 4  # Batch size (iterate over a bunch of actions per policy to estimate expected return) default 100\n",
    "n_actions = 7  # Choose how many control parameters in pulse/circuit parametrization\n",
    "min_bound_actions = -1.\n",
    "max_bound_actions = 1.\n",
    "observation_space = Box(low=np.array([0, 0]), high=np.array([4 ** n_qubits, tgt_instruction_counts]), shape=(2,),\n",
    "                        seed=seed)\n",
    "action_space = Box(low=min_bound_actions, high=max_bound_actions, shape=(n_actions,), seed=seed)\n",
    "\n",
    "torch_env = TorchQuantumEnvironment(q_env, target_circuit, action_space, observation_space, batch_size=batchsize,\n",
    "                                    training_steps_per_gate=training_steps_per_gate, intermediate_rewards=False,\n",
    "                                    seed=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e20355a7-6530-4459-9e03-d4bbf69daf3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Circuit context\n",
    "seed = 10\n",
    "training_steps_per_gate = 3\n",
    "# target_circuit = transpile(random_circuit(4, depth=2, max_operands=2, seed=seed), backend)\n",
    "target_circuit = QuantumCircuit(2)\n",
    "target_circuit.cx(0,1)\n",
    "\n",
    "target_circuit = transpile(target_circuit, backend, optimization_level=0)\n",
    "tgt_qubits = [target_circuit.qubits[i] for i in physical_qubits]\n",
    "\n",
    "tgt_instruction_counts = target_circuit.data.count(CircuitInstruction(target_gate, tgt_qubits))\n",
    "\n",
    "batchsize = 400  # Batch size (iterate over a bunch of actions per policy to estimate expected return) default 100\n",
    "n_actions = 7  # Choose how many control parameters in pulse/circuit parametrization\n",
    "min_bound_actions = -1.\n",
    "max_bound_actions = 1.\n",
    "observation_space = Box(low=np.array([0, 0]), high=np.array([4 ** n_qubits, tgt_instruction_counts]), shape=(2,),\n",
    "                        seed=seed)\n",
    "action_space = Box(low=min_bound_actions, high=max_bound_actions, shape=(n_actions,), seed=seed)\n",
    "\n",
    "torch_env = TorchQuantumEnvironment(q_env, target_circuit, action_space, observation_space, batch_size=batchsize,\n",
    "                                    training_steps_per_gate=training_steps_per_gate, intermediate_rewards=False,\n",
    "                                    seed=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4b6bdf-18c0-4695-b040-811d3c96a498",
   "metadata": {},
   "source": [
    "# Definition of the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19c5e633-252f-4efe-8490-b82edafd5781",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ActorNetwork(nn.Module):\n",
    "    def __init__(self, observation_space: Space, hidden_layers: Sequence[int],\n",
    "                 n_actions: int,\n",
    "                 hidden_activation_functions: Optional[Sequence[nn.Module]] = None,\n",
    "                 include_critic=True,\n",
    "                 chkpt_dir: str = 'tmp/ppo'):\n",
    "        super(ActorNetwork, self).__init__()\n",
    "\n",
    "        self.checkpoint_dir = chkpt_dir\n",
    "        # Define a list to hold the layer sizes including input and output sizes\n",
    "        layer_sizes = [observation_space.shape[0]] + list(hidden_layers)\n",
    "        if hidden_activation_functions is None:\n",
    "            hidden_activation_functions = [nn.ReLU() for _ in range(len(layer_sizes))]\n",
    "\n",
    "        assert len(hidden_activation_functions) == len(layer_sizes)\n",
    "        # Define a list to hold the layers of the network\n",
    "        layers = []\n",
    "\n",
    "        # Iterate over the layer sizes to create the network layers\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            # Add a linear layer with the current and next layer sizes\n",
    "            layers.append(nn.Linear(layer_sizes[i], layer_sizes[i + 1]))\n",
    "\n",
    "            layers.append(hidden_activation_functions[i])\n",
    "\n",
    "        # Create the actor network using Sequential container\n",
    "        self.layers = layers\n",
    "        self.mean_action = nn.Linear(hidden_layers[-1], n_actions)\n",
    "        self.mean_activation = nn.Tanh()\n",
    "        self.std_action = nn.Linear(hidden_layers[-1], n_actions)\n",
    "        self.std_activation = nn.Sigmoid()\n",
    "\n",
    "        self.include_critic = include_critic\n",
    "        self.critic_output = nn.Linear(hidden_layers[-1], 1)\n",
    "\n",
    "        self.base_network = nn.Sequential(*layers)\n",
    "\n",
    "        # Initialize the weights of the network\n",
    "        for layer in self.base_network.modules():\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.orthogonal_(layer.weight)\n",
    "                nn.init.constant_(layer.bias, 0.)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.base_network(x)\n",
    "        mean_action = self.mean_action(x)\n",
    "        mean_action = self.mean_activation(mean_action)\n",
    "        std_action = self.std_action(x)\n",
    "        std_action = self.std_activation(std_action)\n",
    "        critic_output = self.critic_output(x)\n",
    "\n",
    "        if self.critic_output:\n",
    "            return mean_action, std_action, critic_output\n",
    "        else:\n",
    "            return mean_action, std_action\n",
    "\n",
    "    def get_value(self, x):\n",
    "        x = self.base_network(x)\n",
    "        x = self.critic_output(x)\n",
    "        return x\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        torch.save(self, self.checkpoint_dir)\n",
    "\n",
    "    def load_checkpoint(self):\n",
    "        torch.load(self.checkpoint_dir)\n",
    "\n",
    "\n",
    "class CriticNetwork(nn.Module):\n",
    "    def __init__(self, observation_space: Space, hidden_layers: Sequence[int],\n",
    "                 hidden_activation_functions: Optional[Sequence[nn.Module]] = None,\n",
    "                 chkpt_dir: str = 'tmp/critic_ppo'):\n",
    "        super(CriticNetwork, self).__init__()\n",
    "        self.checkpoint_dir = chkpt_dir\n",
    "        # Define a list to hold the layer sizes including input and output sizes\n",
    "        layer_sizes = [observation_space.shape[0]] + list(hidden_layers)\n",
    "        if hidden_activation_functions is None:\n",
    "            hidden_activation_functions = [nn.ReLU() for _ in range(len(layer_sizes))]\n",
    "\n",
    "        assert len(hidden_activation_functions) == len(layer_sizes)\n",
    "        # Define a list to hold the layers of the network\n",
    "        layers = []\n",
    "\n",
    "        # Iterate over the layer sizes to create the network layers\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            # Add a linear layer with the current and next layer sizes\n",
    "            layers.append(nn.Linear(layer_sizes[i], layer_sizes[i + 1]))\n",
    "\n",
    "            # Add a ReLU activation function for all layers except the output layer\n",
    "\n",
    "            layers.append(hidden_activation_functions[i])\n",
    "\n",
    "        # Create the actor network using Sequential container\n",
    "        self.layers = layers\n",
    "        self.critic_output = nn.Linear(hidden_layers[-1], 1)\n",
    "        self.layers.append(self.critic_output)\n",
    "        self.critic_network = nn.Sequential(*self.layers)\n",
    "\n",
    "        # Initialize the weights of the network\n",
    "        for layer in self.critic_network.modules():\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.orthogonal_(layer.weight)\n",
    "                nn.init.constant_(layer.bias, 0.)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.critic_network(x)\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        torch.save(self, self.checkpoint_dir)\n",
    "\n",
    "    def load_checkpoint(self):\n",
    "        torch.load(self.checkpoint_dir)\n",
    "\n",
    "\n",
    "class Agent(nn.Module):\n",
    "    def __init__(self, actor_net: ActorNetwork, critic_net: Optional[CriticNetwork]):\n",
    "        super().__init__()\n",
    "\n",
    "        self.actor_net = actor_net\n",
    "        self.critic_net = critic_net\n",
    "\n",
    "        if self.critic_net is not None:\n",
    "            assert not self.actor_net.include_critic, \"Critic already included in Actor Network\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.actor_net.include_critic:\n",
    "            return self.actor_net(x)\n",
    "        else:\n",
    "            assert self.critic_net is not None, 'Critic Network not provided and not included in ActorNetwork'\n",
    "            mean_action, std_action = self.actor_net(x)\n",
    "            value = self.critic_net(x)\n",
    "            return mean_action, std_action, value\n",
    "\n",
    "    def get_value(self, x):\n",
    "        if self.actor_net.include_critic:\n",
    "            return self.actor_net.get_value(x)\n",
    "        else:\n",
    "            assert self.critic_net is not None, 'Critic Network not provided and not included in ActorNetwork'\n",
    "            return self.critic_net(x)\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        self.actor_net.save_checkpoint()\n",
    "        if self.critic_net is not None:\n",
    "            self.critic_net.save_checkpoint()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3aee453-8139-47dd-8409-88aa72c31181",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "actor_net = ActorNetwork(observation_space, [128, 128], n_actions, [nn.ELU(), nn.ELU(), nn.ELU()]).to(device)\n",
    "critic_net = CriticNetwork(observation_space, [128, 128], [nn.ELU(), nn.ELU(), nn.ELU()]).to(device)\n",
    "agent = Agent(actor_net, critic_net=None).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c979c9-8d16-42fa-88d3-8810dc0f5758",
   "metadata": {},
   "source": [
    "## Hyperparameters for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa0544ad-7134-41f4-bc5e-4197c5750d4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "-----------------------------------------------------------------------------------------------------\n",
    "Hyperparameters for RL agent\n",
    "-----------------------------------------------------------------------------------------------------\n",
    "\"\"\"\n",
    "run_name = \"test\"\n",
    "writer = SummaryWriter(f\"runs/{run_name}\")\n",
    "# writer.add_text(\n",
    "#     \"hyperparameters\",\n",
    "#     \"|param|value|\\n|-|-|\\n%s\" % (\"\\n\".join([f\"|{key}|{value}|\" for key, value in vars(args).items()])),\n",
    "# )\n",
    "# Hyperparameters for the agent\n",
    "n_epochs = 10  # Number of epochs : default 1500\n",
    "num_updates = 1000\n",
    "opti = \"Adam\"\n",
    "lr_actor = 0.01  # Learning rate for policy update step\n",
    "lr_critic = 0.0018  # Learning rate for critic (value function) update step\n",
    "\n",
    "epsilon = 0.2  # Parameter for clipping value (PPO)\n",
    "critic_loss_coeff = 0.5\n",
    "optimizer = optim.Adam(agent.parameters(), lr=lr_actor, eps=1e-5)\n",
    "actor_optimizer = optim.Adam(actor_net.parameters(), lr=lr_actor, eps=1e-5)\n",
    "critic_optimizer = optim.Adam(critic_net.parameters(), lr=lr_critic, eps=1e-5)\n",
    "minibatch_size = 40\n",
    "gamma = 1.\n",
    "gae_lambda = 0.95\n",
    "\n",
    "# Clipping\n",
    "clip_vloss = True\n",
    "grad_clip = 0.5\n",
    "clip_coef = 0.5\n",
    "normalize_advantage = False\n",
    "\n",
    "# other coefficients\n",
    "ent_coef = 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce12ee7-d62f-471a-8596-ef884dfa2a46",
   "metadata": {},
   "source": [
    "# Training\n",
    "## Storage setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0df2e66-a289-4e87-8df6-bf9b5a7419c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "global_step = 0\n",
    "torch_env.reset_global_step()\n",
    "obs = torch.zeros((tgt_instruction_counts, batchsize) + torch_env.observation_space.shape).to(device)\n",
    "actions = torch.zeros((tgt_instruction_counts, batchsize) + torch_env.action_space.shape).to(device)\n",
    "logprobs = torch.zeros((tgt_instruction_counts, batchsize)).to(device)\n",
    "rewards = torch.zeros((tgt_instruction_counts, batchsize)).to(device)\n",
    "dones = torch.zeros((tgt_instruction_counts, batchsize)).to(device)\n",
    "values = torch.zeros((tgt_instruction_counts, batchsize)).to(device)\n",
    "\n",
    "train_obs = torch.zeros((batchsize,) + torch_env.observation_space.shape, requires_grad=True).to(device)\n",
    "visualization_steps = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10306c6a-de03-4ead-bc32-ffdcb1c0da20",
   "metadata": {},
   "source": [
    "## Main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c98ce6ee-b798-42aa-826d-eb773d051bbb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                  | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting benchmarking...\n",
      "Finished benchmarking\n",
      "Running Estimator\n",
      "Job done\n",
      "mean "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                        | 1/1000 [00:07<2:09:59,  7.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0189,  0.0854, -0.0222,  0.1979,  0.1517, -0.0345,  0.0371])\n",
      "sigma tensor([0.5142, 0.5115, 0.5023, 0.5160, 0.4404, 0.4390, 0.5358])\n",
      "Average return: 0.6657352900446876\n",
      "Circuit fidelity: 0.3210938745303728\n",
      "Starting benchmarking...\n",
      "Finished benchmarking\n",
      "Running Estimator\n",
      "Job done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                        | 2/1000 [00:09<1:06:49,  4.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean tensor([-0.2118,  0.5744,  0.2138,  0.7260,  0.5977, -0.0872, -0.1033])\n",
      "sigma tensor([0.0387, 0.4824, 0.6198, 0.5941, 0.3122, 0.1387, 0.5979])\n",
      "Average return: 0.4896994823995652\n",
      "Circuit fidelity: 0.4324814148094814\n",
      "Starting benchmarking...\n",
      "Finished benchmarking\n",
      "Running Estimator\n",
      "Job done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▏                                         | 3/1000 [00:10<48:59,  2.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean tensor([-0.5861,  0.2812, -0.0877,  0.3270,  0.4334, -0.1611,  0.0753])\n",
      "sigma tensor([0.1718, 0.4783, 0.6393, 0.4473, 0.5218, 0.2450, 0.6294])\n",
      "Average return: 0.5624521320859768\n",
      "Circuit fidelity: 0.1869938513989894\n",
      "Starting benchmarking...\n",
      "Finished benchmarking\n",
      "Running Estimator\n",
      "Job done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▏                                         | 4/1000 [00:12<39:25,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean tensor([ 0.8965,  0.1173, -0.4240,  0.6606,  0.2262, -0.4184, -0.1954])\n",
      "sigma tensor([0.0502, 0.6104, 0.4138, 0.3268, 0.2101, 0.2937, 0.6257])\n",
      "Average return: 0.49492892824509743\n",
      "Circuit fidelity: 0.016019454179642616\n",
      "Starting benchmarking...\n",
      "Finished benchmarking\n",
      "Running Estimator\n",
      "Job done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arthurstrauss/opt/anaconda3/envs/qiskit_env/lib/python3.9/site-packages/qiskit_aer/primitives/estimator.py:446: RuntimeWarning: invalid value encountered in sqrt\n",
      "  standard_error = np.sqrt(combined_var / shots)\n",
      "  0%|▏                                         | 4/1000 [00:13<56:22,  3.40s/it]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected parameter loc (Tensor of shape (40, 7)) of distribution Normal(loc: torch.Size([40, 7]), scale: torch.Size([40, 7])) to satisfy the constraint Real(), but found invalid values:\ntensor([[nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan]], grad_fn=<TanhBackward0>)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     66\u001b[0m mb_inds \u001b[38;5;241m=\u001b[39m b_inds[start:end]\n\u001b[1;32m     67\u001b[0m new_mean, new_sigma, new_value \u001b[38;5;241m=\u001b[39m agent(b_obs[mb_inds])\n\u001b[0;32m---> 68\u001b[0m new_dist \u001b[38;5;241m=\u001b[39m \u001b[43mNormal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_sigma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m new_logprob, entropy \u001b[38;5;241m=\u001b[39m new_dist\u001b[38;5;241m.\u001b[39mlog_prob(b_actions[mb_inds])\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m1\u001b[39m), new_dist\u001b[38;5;241m.\u001b[39mentropy()\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     70\u001b[0m logratio \u001b[38;5;241m=\u001b[39m new_logprob \u001b[38;5;241m-\u001b[39m b_logprobs[mb_inds]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/qiskit_env/lib/python3.9/site-packages/torch/distributions/normal.py:56\u001b[0m, in \u001b[0;36mNormal.__init__\u001b[0;34m(self, loc, scale, validate_args)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     55\u001b[0m     batch_shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloc\u001b[38;5;241m.\u001b[39msize()\n\u001b[0;32m---> 56\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/qiskit_env/lib/python3.9/site-packages/torch/distributions/distribution.py:62\u001b[0m, in \u001b[0;36mDistribution.__init__\u001b[0;34m(self, batch_shape, event_shape, validate_args)\u001b[0m\n\u001b[1;32m     60\u001b[0m         valid \u001b[38;5;241m=\u001b[39m constraint\u001b[38;5;241m.\u001b[39mcheck(value)\n\u001b[1;32m     61\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m valid\u001b[38;5;241m.\u001b[39mall():\n\u001b[0;32m---> 62\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     63\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected parameter \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     64\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(value)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtuple\u001b[39m(value\u001b[38;5;241m.\u001b[39mshape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     65\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof distribution \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     66\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto satisfy the constraint \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(constraint)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     67\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut found invalid values:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     68\u001b[0m             )\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[0;31mValueError\u001b[0m: Expected parameter loc (Tensor of shape (40, 7)) of distribution Normal(loc: torch.Size([40, 7]), scale: torch.Size([40, 7])) to satisfy the constraint Real(), but found invalid values:\ntensor([[nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan]], grad_fn=<TanhBackward0>)"
     ]
    }
   ],
   "source": [
    "for update in tqdm.tqdm(range(1, num_updates + 1)):\n",
    "    next_obs, _ = torch_env.reset(seed=seed)\n",
    "    num_steps = torch_env.episode_length(global_step)\n",
    "    next_obs = torch.Tensor(np.array([next_obs] * batchsize)).to(device)\n",
    "    next_done = torch.zeros(batchsize).to(device)\n",
    "\n",
    "    # print(\"episode length:\", num_steps)\n",
    "\n",
    "    for step in range(num_steps):\n",
    "        global_step += 1\n",
    "        obs[step] = next_obs\n",
    "        dones[step] = next_done\n",
    "\n",
    "        with torch.no_grad():\n",
    "            mean_action, std_action, critic_value = agent(next_obs)\n",
    "            probs = Normal(mean_action, std_action)\n",
    "            action = torch.clip(probs.sample(), min_bound_actions, max_bound_actions)\n",
    "            logprob = probs.log_prob(action).sum(1)\n",
    "            values[step] = critic_value.flatten()\n",
    "\n",
    "        actions[step] = action\n",
    "        logprobs[step] = logprob\n",
    "        # next_obs, reward, terminated, truncated, infos = torch_env.step(action.cpu().numpy())\n",
    "        next_obs, reward, terminated, truncated, infos = torch_env.step(action.cpu().numpy())\n",
    "        done = np.logical_or(terminated, truncated)\n",
    "        rewards[step] = torch.tensor(reward).to(device)\n",
    "        next_obs = torch.Tensor(np.array([next_obs] * batchsize)).to(device)\n",
    "        next_done = torch.Tensor(np.array([int(done)] * batchsize)).to(device)\n",
    "        # Only print when at least 1 env is done\n",
    "\n",
    "        # print(f\"global_step={global_step}, episodic_return={np.mean(reward)}\")\n",
    "        writer.add_scalar(\"charts/episodic_return\", np.mean(reward), global_step)\n",
    "        writer.add_scalar(\"charts/episodic_length\", num_steps, global_step)\n",
    "\n",
    "    # bootstrap value if not done\n",
    "    with torch.no_grad():\n",
    "        next_value = agent.get_value(next_obs).reshape(1, -1)\n",
    "        advantages = torch.zeros_like(rewards).to(device)\n",
    "        lastgaelam = 0\n",
    "        for t in reversed(range(num_steps)):\n",
    "            if t == num_steps - 1:\n",
    "                nextnonterminal = 1.0 - next_done\n",
    "                nextvalues = next_value\n",
    "            else:\n",
    "                nextnonterminal = 1.0 - dones[t + 1]\n",
    "                nextvalues = values[t + 1]\n",
    "            delta = rewards[t] + gamma * nextvalues * nextnonterminal - values[t]\n",
    "            advantages[t] = lastgaelam = delta + gamma * gae_lambda * nextnonterminal * lastgaelam\n",
    "        returns = advantages + values\n",
    "\n",
    "    # flatten the batch\n",
    "    b_obs = obs.reshape((-1,) + torch_env.observation_space.shape)\n",
    "    b_logprobs = logprobs.reshape(-1)\n",
    "    b_actions = actions.reshape((-1,) + torch_env.action_space.shape)\n",
    "    b_advantages = advantages.reshape(-1)\n",
    "    b_returns = returns.reshape(-1)\n",
    "    b_values = values.reshape(-1)\n",
    "\n",
    "    # Optimizing the policy and value network\n",
    "    b_inds = np.arange(batchsize)\n",
    "    clipfracs = []\n",
    "    for epoch in range(n_epochs):\n",
    "        np.random.shuffle(b_inds)\n",
    "        for start in range(0, batchsize, minibatch_size):\n",
    "            end = start + minibatch_size\n",
    "            mb_inds = b_inds[start:end]\n",
    "            new_mean, new_sigma, new_value = agent(b_obs[mb_inds])\n",
    "            new_dist = Normal(new_mean, new_sigma)\n",
    "            new_logprob, entropy = new_dist.log_prob(b_actions[mb_inds]).sum(1), new_dist.entropy().sum(1)\n",
    "            logratio = new_logprob - b_logprobs[mb_inds]\n",
    "            ratio = logratio.exp()\n",
    "            #print('new_logprob', new_logprob)\n",
    "            #print('b_logprobs[mb_inds]', b_logprobs[mb_inds])\n",
    "            #print(\"logratio\", logratio)\n",
    "            #print('ratio', ratio)\n",
    "            with torch.no_grad():\n",
    "                # calculate approx_kl http://joschu.net/blog/kl-approx.html\n",
    "                old_approx_kl = (-logratio).mean()\n",
    "                approx_kl = ((ratio - 1) - logratio).mean()\n",
    "                clipfracs += [((ratio - 1.0).abs() > epsilon).float().mean().item()]\n",
    "\n",
    "            mb_advantages = b_advantages[mb_inds]\n",
    "            if normalize_advantage:  # Normalize advantage\n",
    "                mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)\n",
    "\n",
    "            # Policy loss\n",
    "            pg_loss1 = -mb_advantages * ratio\n",
    "            pg_loss2 = -mb_advantages * torch.clamp(ratio, 1 - epsilon, 1 + epsilon)\n",
    "            pg_loss = torch.max(pg_loss1, pg_loss2).mean()\n",
    "\n",
    "            # Value loss\n",
    "            newvalue = new_value.view(-1)\n",
    "            if clip_vloss:\n",
    "                v_loss_unclipped = (newvalue - b_returns[mb_inds]) ** 2\n",
    "                v_clipped = b_values[mb_inds] + torch.clamp(\n",
    "                    newvalue - b_values[mb_inds],\n",
    "                    -clip_coef,\n",
    "                    clip_coef,\n",
    "                )\n",
    "                v_loss_clipped = (v_clipped - b_returns[mb_inds]) ** 2\n",
    "                v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)\n",
    "                v_loss = 0.5 * v_loss_max.mean()\n",
    "            else:\n",
    "                v_loss = 0.5 * ((newvalue - b_returns[mb_inds]) ** 2).mean()\n",
    "\n",
    "            entropy_loss = entropy.mean()\n",
    "            loss = pg_loss - ent_coef * entropy_loss + v_loss * critic_loss_coeff\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(agent.parameters(), grad_clip)\n",
    "            optimizer.step()\n",
    "\n",
    "    y_pred, y_true = b_values.cpu().numpy(), b_returns.cpu().numpy()\n",
    "    var_y = np.var(y_true)\n",
    "    explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y\n",
    "    print(\"mean\", mean_action[0])\n",
    "    print(\"sigma\", std_action[0])\n",
    "    print(\"Average return:\", np.mean(torch_env.reward_history, axis=1)[-1])\n",
    "    # print(np.mean(torch_env.reward_history, axis =1)[-1])\n",
    "    print(\"Circuit fidelity:\", torch_env.circuit_fidelity_history[-1])\n",
    "    # TRY NOT TO MODIFY: record rewards for plotting purposes\n",
    "    writer.add_scalar(\"charts/learning_rate\", optimizer.param_groups[0][\"lr\"], global_step)\n",
    "    writer.add_scalar(\"losses/value_loss\", v_loss.item(), global_step)\n",
    "    writer.add_scalar(\"losses/avg_return\", np.mean(torch_env.reward_history, axis=1)[-1], global_step)\n",
    "    # writer.add_scalar(\"losses/avg_gate_fidelity\", torch_env.avg_fidelity_history[-1], global_step)\n",
    "    writer.add_scalar(\"losses/circuit_fidelity\", torch_env.circuit_fidelity_history[-1], global_step)\n",
    "    writer.add_scalar(\"losses/policy_loss\", pg_loss.item(), global_step)\n",
    "    writer.add_scalar(\"losses/entropy\", entropy_loss.item(), global_step)\n",
    "    writer.add_scalar(\"losses/old_approx_kl\", old_approx_kl.item(), global_step)\n",
    "    writer.add_scalar(\"losses/approx_kl\", approx_kl.item(), global_step)\n",
    "    writer.add_scalar(\"losses/clipfrac\", np.mean(clipfracs), global_step)\n",
    "    writer.add_scalar(\"losses/explained_variance\", explained_var, global_step)\n",
    "    if global_step%visualization_steps == 0:\n",
    "        clear_output(wait=True) # for animation\n",
    "\n",
    "torch_env.close()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea0e566-e5f1-41b5-8c4f-b02d5b6fb3bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
