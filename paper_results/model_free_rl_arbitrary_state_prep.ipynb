{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e54a6df",
   "metadata": {},
   "source": [
    "# Arbitrary state preparation using Model-Free Reinforcement Learning\n",
    "\n",
    "This notebook showcases an application of the formalism introduced in PhysRevX.12.011059 (https://doi.org/10.1103/PhysRevX.12.011059) on arbitrary qubit state preparation, as depicted in the Appendix D.2b.\n",
    "\n",
    "The implementation of the quantum environment is done here via Qiskit, calling Qiskit Runtime for the execution of parametrized quantum circuits.\n",
    "\n",
    "Author of notebook: Arthur Strauss\n",
    "\n",
    "Created on 11/11/2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd6258cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-22 15:41:26.985003: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Qiskit imports for building RL environment (circuit level)\n",
    "import numpy as np\n",
    "from qiskit import QuantumCircuit, QuantumRegister, IBMQ, ClassicalRegister\n",
    "from qiskit.circuit import Parameter, ParameterVector, ParameterExpression\n",
    "from qiskit.quantum_info import DensityMatrix, Pauli, Statevector, state_fidelity, SparsePauliOp\n",
    "from qiskit_ibm_runtime import QiskitRuntimeService, Session, Options, IBMBackend\n",
    "from qiskit.primitives import Estimator\n",
    "\n",
    "\n",
    "# Tensorflow imports for building RL agent and framework\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.layers import Dense, Input\n",
    "from tensorflow.python.keras import Sequential, Model\n",
    "from tensorflow.python.keras.optimizer_v2.adam import Adam\n",
    "from tensorflow.python.keras.optimizer_v2.gradient_descent import SGD\n",
    "from tensorflow_probability.python.distributions import MultivariateNormalDiag, Categorical\n",
    "from tf_agents.environments.py_environment import PyEnvironment\n",
    "from tf_agents.environments.tf_environment import TFEnvironment\n",
    "\n",
    "from tensorflow.python.keras.callbacks import TensorBoard\n",
    "# Additional imports\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Union, Dict\n",
    "import csv\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "125ce77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IBMQ.save_account(TOKEN)\n",
    "IBMQ.load_account()  # Load account from disk\n",
    "provider = IBMQ.get_provider(hub='ibm-q', group='open', project='main')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ed3cb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "def generate_pauli_ops(n_qubits: int):\n",
    "    Pauli_ops = [\n",
    "        {\"name\": ''.join(s),\n",
    "         \"matrix\": Pauli(''.join(s)).to_matrix()\n",
    "         }\n",
    "        for s in product([\"I\", \"X\", \"Y\", \"Z\"], repeat=n_qubits)\n",
    "    ]\n",
    "    return Pauli_ops\n",
    "\n",
    "def apply_parametrized_circuit(qc: QuantumCircuit):\n",
    "    \"\"\"\n",
    "    Define ansatz circuit to be played on Quantum Computer. Should be parametrized with Qiskit ParameterVector\n",
    "    :param qc: Quantum Circuit instance to add the gates on\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    qc.num_qubits\n",
    "    params = ParameterVector('theta', 2)\n",
    "    qc.rx(params[0], 0)\n",
    "    qc.ry(params[1], 1)\n",
    "    # qc.u(angle[0][0], angle[0][1], angle[0, 2], 0)\n",
    "    # qc.u(angle[1][0], angle[1][1], angle[1, 2], 1)\n",
    "    # qc.ecr(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74614f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define RL Environment: the Quantum system of interest\n",
    "\n",
    "class QuantumEnvironment:  # TODO: Build a PyEnvironment out of it\n",
    "    def __init__(self, n_qubits: int, target_state: Dict[str, Union[str, DensityMatrix]], abstraction_level: str,\n",
    "                 backend: IBMBackend, sampling_Pauli_space: int = 10, n_shots: int = 1, c_factor: float = 0.5):\n",
    "        \"\"\"\n",
    "        Class for building quantum environment for RL agent aiming to perform a state preparation task.\n",
    "\n",
    "        :param n_qubits: Number of qubits in quantum system\n",
    "        :param abstraction_level: Circuit or pulse level parametrization of action space\n",
    "        :param backend: Quantum backend, QASM simulator by default\n",
    "        :param target_state: control target of interest\n",
    "        :param sampling_Pauli_space: Number of samples to build fidelity estimator for one action\n",
    "        :param n_shots: Number of shots to sample for one specific computation (action/Pauli expectation sampling)\n",
    "        :param c_factor: Scaling factor for reward definition\n",
    "        \"\"\"\n",
    "\n",
    "        self.c_factor = c_factor\n",
    "        assert abstraction_level == 'circuit' or abstraction_level == 'pulse', 'Abstraction layer parameter can be' \\\n",
    "                                                                               'only pulse or circuit'\n",
    "        self.abstraction_level = abstraction_level\n",
    "        if abstraction_level == 'circuit':\n",
    "            self.q_register = QuantumRegister(n_qubits, name=\"q\")\n",
    "            self.c_register = ClassicalRegister(n_qubits, name=\"c\")\n",
    "            self.qc = QuantumCircuit(self.q_register, self.c_register)\n",
    "            self.backend = backend\n",
    "        else:\n",
    "            # TODO: Define pulse level (Schedule most likely, cf Qiskit Pulse doc)\n",
    "            pass\n",
    "        self.time_step = 0\n",
    "        self.Pauli_ops = generate_pauli_ops(n_qubits)\n",
    "        self.d = 2 ** n_qubits  # Dimension of Hilbert space\n",
    "        self.density_matrix = np.zeros([self.d, self.d], dtype='complex128')\n",
    "        self.sampling_Pauli_space = sampling_Pauli_space\n",
    "        self.n_shots = n_shots\n",
    "        self.target_state = self.calculate_chi_target_state(target_state)\n",
    "\n",
    "    def calculate_chi_target_state(self, target_state: Dict):\n",
    "        \"\"\"\n",
    "        Calculate for all P\n",
    "        :param target_state: Dictionary containing info on target state (name, density matrix)\n",
    "        :return: target state, initializes self.target_state argument\n",
    "        \"\"\"\n",
    "        # target_state[\"Chi\"] = np.zeros(self.d ** 2, dtype=\"complex_\")\n",
    "        assert np.imag([np.array(target_state[\"dm\"].to_operator())\n",
    "                        @ self.Pauli_ops[k][\"matrix\"] for k in\n",
    "                        range(self.d ** 2)]).all() == 0.\n",
    "        target_state[\"Chi\"] = np.array([np.trace(np.array(target_state[\"dm\"].to_operator())\n",
    "                                                 @ self.Pauli_ops[k][\"matrix\"]).real / np.sqrt(self.d) for k in\n",
    "                                        range(\n",
    "                                            self.d ** 2)])  # Real part is taken to convert it in a good format, but im\n",
    "        # is 0 systematically as dm is hermitian and Pauli is traceless\n",
    "        return target_state\n",
    "\n",
    "    def perform_action(self, actions: Union[tf.Tensor, np.array]):\n",
    "        \"\"\"\n",
    "        Execute quantum circuit with parametrized amplitude, retrieve measurement result and assign rewards accordingly\n",
    "        :param actions: action vector to execute on quantum system\n",
    "        :return: Reward table (reward for each run in the batch), observations (measurement outcomes),\n",
    "        obtained density matrix\n",
    "        \"\"\"\n",
    "        global options\n",
    "        angles, batch_size = np.array(actions), len(np.array(actions))\n",
    "\n",
    "        # Direct fidelity estimation protocol  (https://doi.org/10.1103/PhysRevLett.106.230501)\n",
    "        distribution = Categorical(probs=self.target_state[\"Chi\"] ** 2)\n",
    "        k_samples = distribution.sample(self.sampling_Pauli_space)\n",
    "        pauli_index, _, pauli_shots = tf.unique_with_counts(k_samples)\n",
    "\n",
    "        reward_factor = np.round([self.c_factor * self.target_state[\"Chi\"][p] / (self.d * distribution.prob(p))\n",
    "                                  for p in pauli_index], 5)\n",
    "        observables = [SparsePauliOp(self.Pauli_ops[p][\"name\"]) for p in pauli_index]\n",
    "\n",
    "        # print(type(self.target_state[\"Chi\"]))\n",
    "        # print(\"drawn Pauli operators to sample\", k_samples)\n",
    "        # print(pauli_index, pauli_shots)\n",
    "        # print([distribution.prob(p) for p in pauli_index])\n",
    "        # print(observables)\n",
    "        # Perform actions, followed by relevant expectation value sampling for reward calculation\n",
    "\n",
    "        # Apply parametrized quantum circuit (action)\n",
    "        apply_parametrized_circuit(self.qc)\n",
    "\n",
    "        # Keep track of state for benchmarking purposes only\n",
    "        self.density_matrix = np.zeros([self.d, self.d], dtype='complex128')\n",
    "        for angle_set in angles:\n",
    "            qc_2 = self.qc.bind_parameters(angle_set)\n",
    "            q_state = Statevector.from_instruction(qc_2)\n",
    "            self.density_matrix += np.array(q_state.to_operator())\n",
    "        self.density_matrix /= batch_size\n",
    "\n",
    "        total_shots = self.n_shots * pauli_shots\n",
    "        job_list = []\n",
    "        result_list = []\n",
    "        exp_values = np.zeros((len(pauli_index), batch_size))\n",
    "        with Session(service=service, backend=self.backend) as session:\n",
    "            estimator = Estimator(options=options)  # TODO: Figure how to put in the options\n",
    "            for p in range(len(pauli_index)):\n",
    "                job = estimator.run(circuits=[self.qc] * batch_size, observables=[observables[p]] * batch_size,\n",
    "                                    parameter_values=angles,\n",
    "                                    shots=int(total_shots[p]))\n",
    "                job_list.append(job)\n",
    "                result_list.append(job.result())\n",
    "                exp_values[p] = result_list[p].values\n",
    "\n",
    "        self.qc.clear()\n",
    "\n",
    "        reward_table = np.mean(reward_factor[:, np.newaxis] * exp_values, axis=0)\n",
    "        assert len(reward_table) == batch_size\n",
    "        return reward_table, DensityMatrix(self.density_matrix)  # Shape [batchsize]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a85f896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<IBMBackend('ibmq_qasm_simulator')>\n",
      "{'dm': DensityMatrix([[0.5+0.j, 0. +0.j, 0. +0.j, 0.5+0.j],\n",
      "               [0. +0.j, 0. +0.j, 0. +0.j, 0. +0.j],\n",
      "               [0. +0.j, 0. +0.j, 0. +0.j, 0. +0.j],\n",
      "               [0.5+0.j, 0. +0.j, 0. +0.j, 0.5+0.j]],\n",
      "              dims=(2, 2))}\n"
     ]
    }
   ],
   "source": [
    "# Variables to define environment\n",
    "# If you need to overwrite the account info, please add `overwrite=True`\n",
    "#QiskitRuntimeService.save_account(channel=\"ibm_quantum\", token=\"my IBM token\")\n",
    "service = QiskitRuntimeService(channel='ibm_quantum')\n",
    "seed = 3590  # Seed for action sampling\n",
    "backend = service.backends(simulator=True)[0]  # Simulation backend (mock quantum computer)\n",
    "print(backend)\n",
    "options = {\"seed_simulator\": 42,\n",
    "           'resilience_level': 0} \n",
    "n_qubits = 2\n",
    "\n",
    "# Define target state\n",
    "\n",
    "ket0 = np.array([[1.], [0]])\n",
    "ket1 = np.array([[0.], [1.]])\n",
    "ket00 = np.kron(ket0, ket0)\n",
    "ket11 = np.kron(ket1, ket1)\n",
    "bell_state = (ket00 + ket11) / np.sqrt(2)\n",
    "bell_dm = bell_state @ bell_state.conj().T\n",
    "bell_tgt = {\"dm\": DensityMatrix(bell_dm)}\n",
    "print(bell_tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87ddad2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-22 15:43:38.884125: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "drawn Pauli operators to sample tf.Tensor([15  0  0  0 10  5  0 10 10 10], shape=(10,), dtype=int32)\n",
      "tf.Tensor([15  0 10  5], shape=(4,), dtype=int32) tf.Tensor([1 4 4 1], shape=(4,), dtype=int32)\n",
      "[<tf.Tensor: shape=(), dtype=float64, numpy=0.25>, <tf.Tensor: shape=(), dtype=float64, numpy=0.25>, <tf.Tensor: shape=(), dtype=float64, numpy=0.25>, <tf.Tensor: shape=(), dtype=float64, numpy=0.25>]\n",
      "[SparsePauliOp(['ZZ'],\n",
      "              coeffs=[1.+0.j]), SparsePauliOp(['II'],\n",
      "              coeffs=[1.+0.j]), SparsePauliOp(['YY'],\n",
      "              coeffs=[1.+0.j]), SparsePauliOp(['XX'],\n",
      "              coeffs=[1.+0.j])]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'QuantumEnvironment' object has no attribute 'q_register'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Trying environment\u001b[39;00m\n\u001b[1;32m      3\u001b[0m q_env \u001b[38;5;241m=\u001b[39m QuantumEnvironment(\u001b[38;5;241m2\u001b[39m, bell_tgt, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcircuit\u001b[39m\u001b[38;5;124m\"\u001b[39m, backend, sampling_Pauli_space\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mq_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mperform_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0.3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.4\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36mQuantumEnvironment.perform_action\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28mprint\u001b[39m(observables)\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# Perform actions, followed by relevant expectation value sampling for reward calculation\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# Apply parametrized quantum circuit (action)\u001b[39;00m\n\u001b[0;32m---> 85\u001b[0m apply_parametrized_circuit(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqc, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mq_register\u001b[49m)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# Keep track of state for benchmarking purposes only\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m angle_set \u001b[38;5;129;01min\u001b[39;00m angles:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'QuantumEnvironment' object has no attribute 'q_register'"
     ]
    }
   ],
   "source": [
    "# Trying environment\n",
    "\n",
    "q_env = QuantumEnvironment(2, bell_tgt, \"circuit\", backend, sampling_Pauli_space=10)\n",
    "q_env.perform_action(np.array([[0.1, 0.2], [0.3, 0.4]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c08a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qiskit_ibm_runtime import QiskitRuntimeService\n",
    "\n",
    "service = QiskitRuntimeService()\n",
    "program_inputs = {'iterations': 1}\n",
    "options = {\"backend_name\": \"ibmq_qasm_simulator\"}\n",
    "job = service.run(program_id=\"hello-world\",\n",
    "                options=options,\n",
    "                inputs=program_inputs\n",
    "                )\n",
    "print(f\"job id: {job.job_id()}\")\n",
    "result = job.result()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987c4bb0",
   "metadata": {},
   "source": [
    "We now define the Agent, which will be in general a Deep Neural Network.\n",
    "We start by defining the hyperparameters of the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0598de5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters for the agent\n",
    "n_epochs = 50  # Number of epochs\n",
    "batchsize = 50  # Batch size (iterate over a bunch of actions per policy to estimate expected return)\n",
    "N_shots = 1  # Number of shots for sampling the quantum computer for each action vector\n",
    "eta = 0.1  # Learning rate for policy update step\n",
    "eta_2 = 0.1  # Learning rate for critic (value function) update step\n",
    "\n",
    "use_PPO = True\n",
    "epsilon = 0.2  # Parameter for clipping value (PPO)\n",
    "grad_clip = 0.3\n",
    "\n",
    "critic_loss_coeff = 0.5\n",
    "\n",
    "\n",
    "def select_optimizer(optimizer: str = \"Adam\", concurrent_optimization: bool = True, grad_clip: float = 0.3):\n",
    "    if concurrent_optimization:\n",
    "        if optimizer == 'Adam':\n",
    "            return tf.optimizers.Adam(learning_rate=eta, clipvalue=grad_clip)\n",
    "        elif optimizer == 'SGD':\n",
    "            return tf.optimizers.SGD(learning_rate=eta, clipvalue=grad_clip)\n",
    "    else:\n",
    "        if optimizer == 'Adam':\n",
    "            return Adam(learning_rate=eta), Adam(learning_rate=eta_2, clipvalue=grad_clip)\n",
    "        elif optimizer == 'SGD':\n",
    "            return SGD(learning_rate=eta), SGD(learning_rate=eta_2, clipvalue=grad_clip)\n",
    "\n",
    "\n",
    "optimizer = select_optimizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2462c0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy parameters\n",
    "\n",
    "N_in = n_qubits + 1  # One input for each measured qubit state (0 or 1 input for each neuron)\n",
    "n_actions = 1  # Choose how many control parameters in pulse/circuit parametrization\n",
    "N_out = 2 * n_actions  # One mean/variance for each action\n",
    "layers = [3]  # List containing the number of neurons in each hidden layer\n",
    "\n",
    "input_layer = Input(shape=N_in)\n",
    "hidden = Sequential([Dense(layer, activation='relu', kernel_initializer=tf.initializers.RandomNormal(stddev=0.01),\n",
    "                           bias_initializer=tf.initializers.RandomNormal(stddev=0.01))\n",
    "                     for layer in layers])(input_layer)\n",
    "actor_output = Dense(N_out, activation=None)(hidden)\n",
    "critic_output = Dense(1, activation=None)(hidden)\n",
    "network = Model(inputs=input_layer, outputs=[actor_output, critic_output])\n",
    "init_msmt = np.zeros([1, N_in])\n",
    "\n",
    "sigma_eps = 1e-6  # for numerical stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4a7b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_old = tf.Variable(initial_value=network(init_msmt)[0][0][:N_out // 2], trainable=False)\n",
    "sigma_old = tf.Variable(initial_value=network(init_msmt)[0][0][N_out // 2:], trainable=False)\n",
    "\n",
    "# Training loop (not working yet)\n",
    "\n",
    "for i in tqdm(range(n_epochs)):\n",
    "    \n",
    "    Policy_distrib = MultivariateNormalDiag(loc=network(init_msmt)[0][0][:N_out // 2],\n",
    "                                            scale_diag=network(init_msmt)[0][0][N_out // 2:], \n",
    "                                            validate_args=True,\n",
    "                                            allow_nan_stats=False)\n",
    "    Old_distrib = MultivariateNormalDiag(loc=mu_old, scale_diag=sigma_old, validate_args=True,\n",
    "                                         allow_nan_stats=False)\n",
    "    \n",
    "    action_vector = Policy_distrib.sample(batchsize, seed=seed)\n",
    "    \n",
    "    # Run quantum circuit to retrieve rewards (in this example, only one time step)\n",
    "    reward, dm_observed = q_env.perform_action(action_vector)\n",
    "    \n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        advantage = reward - network(init_msmt)[1]\n",
    "        \n",
    "        if use_PPO:\n",
    "            ratio = Policy_distrib.prob(action_vector) / (Old_distrib.prob(action_vector) + sigma_eps)\n",
    "            actor_loss = - tf.reduce_mean(tf.minimum(advantage * ratio,\n",
    "                                                     advantage * tf.clip_by_value(ratio, 1 - epsilon, 1 + epsilon)))\n",
    "        else:  # REINFORCE algorithm\n",
    "            actor_loss = - tf.reduce_mean(advantage * Policy_distrib.log_prob(action_vector))\n",
    "\n",
    "        critic_loss = tf.reduce_mean(advantage ** 2)\n",
    "        combined_loss = actor_loss + 0.5 * critic_loss\n",
    "        \n",
    "    grads = tape.gradient(combined_loss, network.trainable_variables)\n",
    "    \n",
    "    # For PPO, update old parameters to have access to \"old\" policy\n",
    "    if use_PPO:\n",
    "        mu_old.assign(Policy_distrib.loc)\n",
    "        sigma_old.assign(network(init_msmt)[0][0][N_out // 2:])\n",
    "        \n",
    "    optimizer.apply_gradients(zip(grads, network.trainable_variables))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
